{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch nn.Module",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dejanbatanjac/pytorch-learning-101/blob/master/PyTorch_nn_Module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2BIFmvo19EO",
        "colab_type": "code",
        "outputId": "a7cb655b-27b0-4064-9afd-a7a8bf6c9688",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Module0(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "    # we don't return a thing \n",
        "    def forward(self)->None: \n",
        "        print(\"Module0:forward\")\n",
        "        \n",
        "# we create a module instance m0\n",
        "m0 = Module0().cuda()\n",
        "\n",
        "# we explicitely run the forward method\n",
        "m0.forward()\n",
        "        \n",
        "m0 = Module0().cuda() \n",
        "m0()\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Module0:forward\n",
            "Module0:forward\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeiQibqG2Q7M",
        "colab_type": "code",
        "outputId": "f630e92c-1b2b-4407-b93e-2f374134b735",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# these are not identical even tough it looks like\n",
        "m0()\n",
        "m0.forward()\n",
        "\n",
        "# we should always call m0()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Module0:forward\n",
            "Module0:forward\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdKa46zZ2NML",
        "colab_type": "code",
        "outputId": "78a17a6a-6e7f-416e-9da8-88c0b4d8c9e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "#Problems\n",
        "\n",
        "# P1. we have no params so we cannot learn\n",
        "# P2. our forward doesn't take any input and doesn't returns any output\n",
        "\n",
        "t = [o.numel() for o in m0.parameters() ]\n",
        "t, sum(t)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([], 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vtj4KNF1V0wP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#fixing the problems P1 and P2 in Module1\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Creating the single linear layer = one matrix tranformation\n",
        "class Module1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(10,100)        \n",
        "        \n",
        "    \n",
        "    def forward(self,x)->None: \n",
        "        print(\"Module1:forward\")\n",
        "        return self.l1(x)\n",
        "        \n",
        "# we create a module instance m1\n",
        "m1 = Module1()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5DnHRlwVsHu",
        "colab_type": "code",
        "outputId": "100f0956-e314-465b-e7e5-397df700e55c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# testing out if we have parameters\n",
        "\n",
        "t = [p.numel() for p in m1.parameters() if p.requires_grad ]\n",
        "print(t, sum(t))\n",
        "\n",
        "#([1000, 100], 1100)\n",
        "# 1000 parameters in W \n",
        "# 100 parameters in b and \n",
        "# 1100 parametes in total\n",
        "\n",
        "# trying some input 10 batches on 10 features\n",
        "\n",
        "inp = torch.empty(10, 10).uniform_()\n",
        "print(inp)\n",
        "out = m1(inp)\n",
        "print(out)\n",
        "#output has 10 batches of 100 features"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1000, 100] 1100\n",
            "tensor([[0.1943, 0.8545, 0.8793, 0.7102, 0.4046, 0.2718, 0.7608, 0.9223, 0.3837,\n",
            "         0.3083],\n",
            "        [0.8162, 0.0176, 0.7427, 0.2288, 0.7135, 0.8049, 0.1336, 0.0455, 0.2951,\n",
            "         0.0237],\n",
            "        [0.1867, 0.6787, 0.9078, 0.3336, 0.3317, 0.3092, 0.1874, 0.0796, 0.3572,\n",
            "         0.7660],\n",
            "        [0.2215, 0.0964, 0.3425, 0.7453, 0.1788, 0.1240, 0.9839, 0.6630, 0.8446,\n",
            "         0.4913],\n",
            "        [0.7615, 0.5373, 0.0317, 0.9994, 0.5345, 0.5920, 0.5245, 0.6511, 0.7182,\n",
            "         0.2406],\n",
            "        [0.8722, 0.4567, 0.7412, 0.3816, 0.2416, 0.4976, 0.5791, 0.0472, 0.6985,\n",
            "         0.1834],\n",
            "        [0.3595, 0.9011, 0.8088, 0.3622, 0.1385, 0.0411, 0.9082, 0.4432, 0.7449,\n",
            "         0.5659],\n",
            "        [0.1207, 0.3547, 0.7430, 0.7472, 0.1694, 0.7854, 0.9672, 0.9447, 0.4042,\n",
            "         0.1316],\n",
            "        [0.5010, 0.8589, 0.1002, 0.3644, 0.4383, 0.9904, 0.5604, 0.5179, 0.9103,\n",
            "         0.0907],\n",
            "        [0.2012, 0.6918, 0.6730, 0.4609, 0.2229, 0.7725, 0.7720, 0.6014, 0.6474,\n",
            "         0.6543]])\n",
            "Module1:forward\n",
            "tensor([[ 1.6086e-01, -1.8648e-01, -7.4761e-01,  8.8757e-01, -5.3465e-02,\n",
            "         -5.2077e-03, -9.7239e-02, -3.5176e-01, -3.9084e-01,  4.3028e-01,\n",
            "         -3.2549e-01,  3.2023e-02, -3.1835e-01,  3.6794e-01, -3.7684e-01,\n",
            "         -2.0079e-01, -6.1020e-02,  2.8843e-01, -3.7281e-01, -8.0387e-02,\n",
            "         -3.7229e-01, -4.1885e-01, -2.4627e-01, -1.4865e-01,  1.2669e-02,\n",
            "          5.2795e-01,  3.5808e-01, -3.5875e-01, -4.0519e-01, -2.1045e-01,\n",
            "          5.0596e-01,  7.2436e-01, -7.7232e-01, -1.8604e-02,  9.8992e-02,\n",
            "          9.5277e-02, -2.1488e-01, -4.9895e-01,  2.1055e-01,  1.4735e-01,\n",
            "          7.3670e-02, -2.8855e-01, -1.4421e-01,  6.7378e-01,  4.3285e-02,\n",
            "         -3.3393e-01,  1.3963e-02,  5.5247e-01, -1.9318e-01,  9.5537e-02,\n",
            "         -4.3752e-01,  3.1581e-01, -3.9026e-01,  3.1305e-01, -1.3419e-01,\n",
            "         -8.0087e-02,  2.6354e-01,  3.3905e-01, -3.3537e-01,  1.3345e-01,\n",
            "         -5.2754e-01, -9.9293e-01, -1.8579e-01, -5.9052e-01, -5.1975e-01,\n",
            "         -5.0375e-02,  4.6222e-03,  1.2913e-01,  3.2790e-01, -8.9457e-02,\n",
            "         -5.3941e-01,  2.3825e-01,  1.9985e-02, -2.5129e-01,  9.0607e-01,\n",
            "         -1.8973e-02, -4.3626e-01,  4.5745e-01, -2.3405e-01,  3.4128e-01,\n",
            "          3.3708e-01, -5.0966e-02, -6.2289e-02, -5.8430e-01, -2.9482e-01,\n",
            "         -1.4251e-02, -6.3316e-02,  6.6819e-02,  1.3758e-01, -3.9898e-01,\n",
            "         -5.6610e-01, -2.1335e-01, -2.8061e-01, -1.4935e-01, -2.6960e-01,\n",
            "         -5.9934e-02,  3.6055e-01, -4.9594e-02,  5.8654e-02,  5.4890e-02],\n",
            "        [ 4.1625e-02, -1.7232e-01, -3.0343e-01,  8.9052e-01, -6.1922e-02,\n",
            "          3.3529e-01, -1.0223e-01, -3.7076e-01,  5.2890e-02, -2.4098e-01,\n",
            "          2.7554e-01, -1.5356e-01,  8.3278e-02,  7.8900e-01, -2.0829e-01,\n",
            "          3.5372e-01,  5.9035e-01,  2.0317e-01, -5.6128e-02,  6.0524e-02,\n",
            "         -7.5313e-02, -6.0978e-01,  1.3161e-02,  1.5927e-02,  7.3549e-02,\n",
            "          3.3056e-01,  5.4900e-01, -1.1765e-01,  2.8394e-01, -2.7556e-01,\n",
            "          4.0923e-01,  6.9581e-01, -9.0730e-01, -3.2914e-02,  9.3923e-02,\n",
            "          1.8876e-01, -1.6652e-01, -4.4159e-01,  4.5182e-01,  2.0706e-01,\n",
            "         -3.2123e-01,  1.3064e-01, -2.4071e-01,  1.0380e-01,  1.5152e-01,\n",
            "         -6.7209e-02, -1.7568e-02,  5.2530e-01, -4.5402e-01, -9.4060e-02,\n",
            "         -2.5384e-01, -1.3596e-01, -3.8634e-01,  2.8310e-01,  1.0712e-01,\n",
            "         -4.2360e-04,  5.3547e-01,  3.7483e-01, -4.6933e-01, -1.2799e-01,\n",
            "         -4.4650e-01, -5.8205e-01, -2.0305e-01, -6.1012e-01,  2.0013e-01,\n",
            "         -8.3796e-02, -2.0369e-01,  2.1112e-01,  3.5053e-01,  6.5824e-01,\n",
            "         -5.2463e-01,  2.6751e-01,  1.6308e-01, -1.2498e-01,  4.7549e-01,\n",
            "         -9.0911e-02, -4.4630e-01,  3.8732e-01, -6.3380e-01,  2.9614e-01,\n",
            "          1.0268e-01,  4.3432e-01, -3.0996e-01, -5.2928e-01, -2.0530e-01,\n",
            "          2.1211e-01, -2.6488e-01,  2.3425e-01, -3.5018e-02, -5.9766e-01,\n",
            "         -4.2801e-01,  2.4739e-01,  1.6686e-01,  2.7741e-01, -5.3762e-01,\n",
            "          3.1531e-01,  5.5195e-01, -4.9585e-01, -1.9371e-01, -1.1396e-01],\n",
            "        [ 2.3498e-01, -1.5471e-01, -6.9771e-01,  9.3782e-01,  7.1731e-02,\n",
            "         -1.7682e-02, -2.8792e-01, -1.4317e-02, -2.5825e-01,  3.1198e-01,\n",
            "         -1.3941e-01, -3.2311e-01, -1.6470e-01,  3.2878e-01, -2.6124e-01,\n",
            "          8.4239e-02,  2.0629e-01,  3.0746e-01, -5.8578e-02,  8.0223e-02,\n",
            "         -3.8255e-01, -2.4456e-01, -2.5441e-01,  2.0365e-01,  1.3864e-01,\n",
            "          4.4403e-01,  3.3514e-01,  8.0827e-02, -2.2400e-01, -1.9565e-01,\n",
            "          2.8926e-01,  5.2550e-01, -2.4426e-01, -2.7094e-01, -3.4431e-01,\n",
            "          1.4801e-01, -4.6169e-01, -2.5463e-01,  1.9469e-01,  1.3575e-01,\n",
            "         -1.5502e-01, -2.3064e-02, -3.2973e-01,  5.3737e-01,  2.0419e-01,\n",
            "         -7.7588e-02,  9.2316e-02,  3.6591e-01, -3.1895e-01, -7.6613e-03,\n",
            "         -4.5603e-01,  1.2749e-01, -2.2675e-01,  2.0636e-01, -1.1808e-01,\n",
            "          4.0916e-02,  4.8133e-01,  3.8399e-02, -2.7727e-01, -4.7263e-02,\n",
            "         -5.1665e-01, -7.9769e-01,  9.8277e-02, -3.0545e-01, -2.2141e-01,\n",
            "         -5.0405e-02, -7.5359e-02, -7.2103e-02,  1.0815e-01,  7.5560e-03,\n",
            "         -3.2280e-01,  4.5928e-01, -1.4556e-02, -5.7474e-02,  7.2412e-01,\n",
            "         -3.3356e-02, -4.6731e-01,  3.1824e-01, -2.0605e-02,  4.0351e-02,\n",
            "          1.8953e-01,  2.8343e-01, -2.3560e-01, -2.4844e-01, -3.9682e-01,\n",
            "         -5.1959e-02, -1.7043e-01,  9.5113e-02,  1.8929e-01, -4.5370e-02,\n",
            "         -6.9618e-01, -9.4071e-03, -8.5074e-02,  1.4249e-01,  1.8989e-02,\n",
            "          3.5260e-01,  4.2775e-01,  3.8036e-02,  5.0010e-02, -3.7171e-01],\n",
            "        [ 2.4609e-01, -6.4663e-02, -7.8862e-01,  7.5496e-01,  3.2766e-02,\n",
            "          2.3327e-01, -2.2392e-01, -1.9961e-01, -3.9374e-01,  1.9629e-01,\n",
            "         -8.2644e-02,  3.0196e-01, -4.3733e-01,  2.9533e-01, -4.3271e-01,\n",
            "          6.1414e-02,  1.0419e-01,  2.2711e-01, -4.4703e-01, -7.2307e-02,\n",
            "         -1.9367e-01, -3.9909e-01, -3.9910e-01,  4.8502e-02, -2.7718e-01,\n",
            "          3.6352e-01, -6.0681e-03, -1.5424e-01, -3.1989e-01, -3.1198e-02,\n",
            "          1.9318e-01,  4.4422e-01, -9.2989e-01, -4.6559e-02,  4.6364e-01,\n",
            "         -3.3533e-01, -4.2118e-01, -2.3019e-01,  1.7239e-01, -1.2821e-01,\n",
            "          7.8670e-02, -2.9920e-02,  1.3418e-01,  6.8549e-01, -1.4190e-01,\n",
            "         -3.4669e-01,  1.6437e-01,  7.0203e-01, -3.3043e-01, -8.3007e-02,\n",
            "         -4.2933e-01,  1.6423e-01, -4.6695e-01,  1.3823e-01, -1.4521e-01,\n",
            "         -1.3664e-01,  2.4114e-01,  4.2148e-02, -5.7352e-01,  7.8512e-02,\n",
            "         -6.9237e-01, -9.4286e-01, -1.7216e-01, -3.8024e-01, -4.0483e-01,\n",
            "         -9.3987e-02,  2.3092e-01,  5.9829e-01,  1.8670e-01,  1.5685e-01,\n",
            "         -3.3565e-01,  1.5235e-01,  3.8859e-01, -6.0832e-01,  7.5588e-01,\n",
            "          2.3638e-01, -1.4240e-01,  8.0226e-01, -1.4390e-01,  3.9747e-02,\n",
            "          1.6767e-01, -4.6375e-01,  4.2876e-02, -3.6948e-01, -6.1895e-02,\n",
            "         -2.6080e-01, -1.0486e-01,  1.9192e-02, -6.4010e-02, -4.1138e-01,\n",
            "         -5.2044e-01, -1.6723e-01, -4.4354e-02, -5.4016e-02, -3.1175e-01,\n",
            "         -2.1451e-01,  6.4324e-02, -1.3213e-01, -1.2144e-01,  4.3538e-01],\n",
            "        [ 2.6251e-01, -3.9546e-01, -3.7788e-01,  9.2021e-01, -5.5711e-02,\n",
            "          2.3209e-01, -5.8099e-01, -7.9005e-01, -1.4224e-01,  1.2063e-01,\n",
            "          4.7118e-02,  5.8048e-02, -5.2594e-01,  3.3503e-01, -3.3591e-01,\n",
            "          7.0916e-02,  2.8407e-01,  4.9110e-01, -1.8258e-01, -1.1251e-01,\n",
            "         -1.9369e-01, -2.9282e-01, -4.6404e-01, -2.3042e-01, -1.3979e-01,\n",
            "          3.0066e-01,  2.1602e-01, -4.9301e-01, -1.2018e-01,  6.3412e-02,\n",
            "          7.9541e-01,  7.0949e-01, -1.1275e+00,  1.8977e-01,  3.5779e-01,\n",
            "         -2.0852e-01, -1.0512e-01, -6.5994e-01,  5.3678e-01,  1.3096e-01,\n",
            "          1.5886e-01, -1.9127e-01, -1.0254e-02,  3.8231e-01, -6.1269e-02,\n",
            "         -3.5090e-01,  2.0725e-01,  6.4823e-01, -3.1823e-01, -7.0568e-02,\n",
            "         -4.5528e-01,  9.6600e-02, -2.5887e-01,  1.5445e-01, -2.9004e-01,\n",
            "          1.2874e-01,  2.7381e-01,  2.2076e-01, -4.3453e-01,  3.0553e-01,\n",
            "         -7.7928e-01, -9.9877e-01, -4.7179e-01, -5.6569e-01, -2.6786e-01,\n",
            "          2.6585e-02,  7.8111e-02,  5.0235e-01,  1.7044e-01,  2.4668e-01,\n",
            "         -6.9056e-01,  1.1658e-01,  3.7940e-01, -4.6919e-01,  6.2128e-01,\n",
            "         -1.5889e-01, -1.8853e-01,  6.8864e-01, -4.1215e-01,  1.6496e-02,\n",
            "          1.6688e-01, -5.6666e-02, -2.3025e-01, -7.1970e-01, -2.5449e-01,\n",
            "          3.4491e-01, -2.1139e-01,  2.2124e-01,  8.5586e-03, -6.6565e-01,\n",
            "         -5.0242e-01, -1.9600e-01,  1.3883e-01, -1.7875e-01, -5.1054e-01,\n",
            "          1.0266e-01,  5.8115e-01, -1.3069e-01, -2.0343e-01,  2.2402e-01],\n",
            "        [ 9.7585e-02, -1.7567e-01, -5.1903e-01,  8.6238e-01,  4.9598e-02,\n",
            "          2.0300e-01, -2.2306e-01, -3.4183e-01, -1.8256e-01, -6.1723e-03,\n",
            "          7.4083e-02, -1.3900e-01, -1.5400e-01,  5.2504e-01, -3.1843e-01,\n",
            "          1.5955e-01,  2.5774e-01,  3.1964e-01,  3.9458e-02, -5.4394e-03,\n",
            "         -1.2127e-01, -5.4010e-01,  3.3069e-02, -1.1720e-01,  6.9521e-02,\n",
            "          5.6521e-01,  5.9255e-01, -2.0698e-01, -6.3456e-02,  2.4435e-02,\n",
            "          2.2850e-01,  8.8483e-01, -8.4518e-01, -6.1368e-02,  2.8097e-01,\n",
            "          2.4271e-02, -2.7330e-01, -4.6655e-01,  4.0223e-01,  1.5305e-01,\n",
            "         -2.9873e-01,  8.7801e-03, -1.5207e-01,  4.7684e-01,  1.8255e-03,\n",
            "         -6.4601e-02,  6.3056e-02,  3.4862e-01, -2.3275e-01, -5.2238e-03,\n",
            "         -3.3401e-01, -1.3789e-01, -3.0447e-01,  1.1972e-01,  4.2889e-02,\n",
            "         -1.0858e-01,  3.8772e-01,  1.7761e-01, -5.1518e-01,  1.0781e-01,\n",
            "         -6.7128e-01, -6.5021e-01, -9.7579e-02, -5.1377e-01, -7.5920e-02,\n",
            "         -3.2353e-01,  1.4602e-02,  2.9304e-01,  2.9905e-01,  4.6912e-01,\n",
            "         -6.5322e-01,  1.9497e-01,  2.3687e-01, -1.7588e-01,  5.5677e-01,\n",
            "         -1.0390e-01, -5.4772e-01,  4.7399e-01, -2.6482e-01,  9.2807e-02,\n",
            "          3.0456e-01,  2.4466e-01, -2.6908e-01, -4.2160e-01, -1.9979e-01,\n",
            "          1.6936e-01, -3.7060e-01,  2.6286e-01,  8.6632e-02, -4.9194e-01,\n",
            "         -6.3441e-01,  1.2703e-01, -1.2609e-01,  2.0344e-01, -4.3662e-01,\n",
            "          2.0403e-01,  4.3262e-01, -3.5054e-01, -1.4967e-01,  9.0984e-02],\n",
            "        [ 2.2895e-01, -1.6629e-01, -8.5338e-01,  8.1833e-01,  1.4097e-01,\n",
            "          3.4689e-02, -1.7391e-01, -1.3759e-01, -3.9827e-01,  3.7646e-01,\n",
            "         -2.4533e-01, -4.7330e-02, -3.4454e-01,  2.0114e-01, -3.6106e-01,\n",
            "         -1.6859e-01, -1.1525e-01,  3.1747e-01, -1.2849e-01,  6.2443e-02,\n",
            "         -3.4781e-01, -3.8800e-01, -1.4324e-01, -1.0089e-02,  6.9079e-03,\n",
            "          6.1531e-01,  4.4398e-01, -1.5173e-01, -4.7335e-01,  1.4256e-02,\n",
            "          1.5432e-01,  6.8974e-01, -5.3201e-01, -1.4586e-01,  7.7756e-02,\n",
            "         -1.6546e-02, -3.5543e-01, -3.8769e-01,  1.8289e-01,  1.3129e-01,\n",
            "         -1.0625e-01, -1.2375e-01, -1.2962e-01,  7.8685e-01, -1.0055e-02,\n",
            "         -1.4376e-01, -1.6903e-02,  3.4096e-01, -9.7158e-02,  4.2698e-02,\n",
            "         -4.4828e-01,  2.0121e-01, -3.4176e-01,  3.7258e-02, -2.4839e-02,\n",
            "         -1.8430e-01,  2.7354e-01,  9.0220e-02, -4.6518e-01,  2.1027e-01,\n",
            "         -7.0630e-01, -8.2552e-01,  1.7948e-02, -4.5521e-01, -4.6069e-01,\n",
            "         -2.9961e-01,  1.8018e-01,  2.4221e-01,  1.9078e-01, -1.1660e-02,\n",
            "         -4.9787e-01,  2.0917e-01,  5.6356e-02, -1.5937e-01,  9.1549e-01,\n",
            "          8.2926e-02, -5.2225e-01,  4.6069e-01,  1.3969e-01,  7.7658e-02,\n",
            "          3.4937e-01, -3.9001e-02, -1.1322e-01, -3.1654e-01, -3.2065e-01,\n",
            "         -1.2631e-01, -1.5737e-01,  1.4355e-01,  2.4021e-01, -2.8565e-01,\n",
            "         -8.3481e-01, -1.5884e-01, -3.7594e-01, -1.7891e-02, -1.9509e-01,\n",
            "          4.8386e-02,  2.9881e-01, -1.2012e-02,  6.9432e-02,  1.3118e-01],\n",
            "        [-9.2291e-02,  4.9602e-02, -6.9665e-01,  7.4988e-01, -2.2373e-01,\n",
            "          6.1321e-02, -1.2565e-01, -3.9049e-01, -5.0548e-01,  1.7643e-01,\n",
            "         -2.9681e-01,  2.2865e-01, -4.5665e-01,  6.2772e-01, -4.8749e-01,\n",
            "         -6.8722e-02,  1.0378e-01,  1.3304e-01, -6.2917e-01, -2.0763e-01,\n",
            "         -3.2699e-01, -6.6443e-01, -1.9581e-01, -2.8498e-01,  3.4150e-02,\n",
            "          5.3933e-01,  2.7758e-01, -4.8154e-01, -3.1076e-01, -2.9980e-01,\n",
            "          4.6897e-01,  8.1777e-01, -1.0921e+00,  1.2048e-02,  5.0774e-01,\n",
            "          1.1532e-01, -1.8522e-01, -2.5643e-01,  2.4807e-01,  1.1478e-01,\n",
            "         -3.8326e-03, -1.2873e-01, -4.8967e-02,  6.1851e-01, -1.5399e-01,\n",
            "         -3.3628e-01,  1.5096e-01,  7.4891e-01, -3.6905e-01, -1.0388e-01,\n",
            "         -5.0665e-01,  1.5931e-01, -4.1920e-01,  4.2756e-01, -1.9439e-01,\n",
            "          3.6019e-02,  3.8649e-01,  3.9453e-01, -4.1614e-01,  1.9790e-01,\n",
            "         -3.6125e-01, -9.8793e-01, -3.0101e-01, -6.0784e-01, -2.8499e-01,\n",
            "         -4.4317e-02, -1.2951e-01,  1.5386e-01,  5.0486e-01,  1.4031e-01,\n",
            "         -5.5119e-01,  1.3326e-01,  3.3111e-01, -5.6710e-01,  7.9077e-01,\n",
            "         -8.2897e-02, -3.8245e-01,  7.0464e-01, -4.7843e-01,  4.1567e-01,\n",
            "          2.1522e-01, -1.8959e-01, -3.1175e-02, -6.9613e-01, -1.7326e-01,\n",
            "         -5.4151e-02,  4.9820e-03, -1.4738e-01, -1.3868e-01, -5.7000e-01,\n",
            "         -2.8127e-01, -1.4144e-02, -1.6956e-01, -6.1596e-02, -4.1251e-01,\n",
            "         -3.2438e-01,  1.5900e-01, -2.4240e-01,  1.6091e-01,  2.2675e-01],\n",
            "        [ 1.0828e-01, -4.2389e-01, -4.4470e-01,  6.9519e-01, -2.0947e-01,\n",
            "          1.0556e-01, -6.0409e-01, -7.9598e-01, -1.5642e-01,  7.9648e-02,\n",
            "          6.6543e-02,  1.3063e-01, -6.8991e-01,  4.0588e-01, -2.7762e-01,\n",
            "          2.0995e-02,  2.2028e-01,  1.8547e-01, -9.9789e-02,  1.8237e-01,\n",
            "         -3.3966e-01, -5.4409e-01, -3.4354e-01, -2.8473e-01,  2.1382e-01,\n",
            "          2.5412e-01,  5.1759e-01, -4.6337e-01, -2.8738e-01,  2.8949e-02,\n",
            "          6.1933e-01,  8.8202e-01, -9.5744e-01,  1.2778e-01,  2.5783e-01,\n",
            "          2.0059e-01,  1.3068e-02, -5.6304e-01,  5.0601e-01,  4.4601e-01,\n",
            "          8.0528e-02,  3.8060e-02, -8.9492e-02,  4.3487e-01, -2.7211e-01,\n",
            "         -2.6169e-01,  1.3228e-01,  3.4093e-01, -4.1942e-01, -3.4275e-01,\n",
            "         -4.8560e-01,  1.6130e-02, -2.7858e-01,  7.1643e-02, -2.2377e-01,\n",
            "          2.2876e-01,  4.0166e-01,  2.3101e-01, -3.0803e-01,  5.4244e-01,\n",
            "         -6.4871e-01, -8.9614e-01, -3.0436e-01, -6.8480e-01, -1.4082e-01,\n",
            "         -1.2041e-01, -1.0099e-01,  2.3123e-01,  2.8561e-01,  2.3831e-01,\n",
            "         -7.7399e-01, -2.6211e-02,  3.6436e-01, -3.3899e-01,  8.6187e-01,\n",
            "         -2.8528e-01, -2.9646e-01,  5.2104e-01, -1.1193e-01,  1.1337e-01,\n",
            "          6.5972e-02,  1.2343e-01, -4.7763e-01, -7.1673e-01, -4.9622e-01,\n",
            "          3.4551e-01,  9.0754e-03,  1.1360e-01,  4.2765e-02, -7.3397e-01,\n",
            "         -6.4359e-01, -2.2318e-01,  1.2612e-01, -2.4174e-01, -4.9907e-01,\n",
            "          7.0481e-02,  6.0106e-01, -9.6756e-02,  1.9181e-01,  9.5738e-02],\n",
            "        [ 2.5915e-02, -5.8796e-02, -7.7695e-01,  8.7093e-01, -9.2870e-02,\n",
            "          6.6994e-02, -4.2119e-01, -2.7767e-01, -4.7445e-01,  2.0462e-01,\n",
            "         -2.4725e-01, -3.1827e-02, -5.8633e-01,  4.0160e-01, -4.0312e-01,\n",
            "         -5.1192e-02,  1.3043e-01,  2.7182e-01, -3.5094e-01, -2.8518e-02,\n",
            "         -4.3193e-01, -5.2357e-01, -3.4709e-01, -8.5739e-02,  9.8952e-02,\n",
            "          5.6380e-01,  3.1360e-01, -3.1183e-01, -3.6888e-01, -2.1863e-01,\n",
            "          4.5350e-01,  7.3040e-01, -7.5758e-01, -1.5470e-01,  1.6129e-01,\n",
            "          1.1815e-01, -2.8883e-01, -2.2770e-01,  2.5944e-01,  2.7728e-01,\n",
            "         -5.6981e-02, -6.1968e-03, -1.7762e-01,  6.0777e-01, -8.2436e-02,\n",
            "         -1.9693e-01,  1.0285e-01,  5.8564e-01, -3.9135e-01, -1.7083e-01,\n",
            "         -6.0079e-01,  1.9586e-01, -3.1017e-01,  2.2341e-01, -2.5470e-01,\n",
            "          9.2673e-02,  5.2350e-01,  2.0628e-01, -4.1559e-01,  3.5390e-01,\n",
            "         -5.2508e-01, -1.0283e+00, -1.5618e-01, -4.8346e-01, -2.7541e-01,\n",
            "         -1.4533e-01, -4.4046e-02,  5.8079e-02,  2.8238e-01,  1.6103e-02,\n",
            "         -4.9262e-01,  2.2636e-01,  2.3409e-01, -3.8542e-01,  9.6501e-01,\n",
            "         -8.5662e-02, -4.6196e-01,  6.2121e-01, -1.3707e-01,  1.4611e-01,\n",
            "          9.6730e-02,  1.5646e-02, -2.0728e-01, -5.0708e-01, -4.1884e-01,\n",
            "         -1.6870e-02,  2.2267e-02, -6.0390e-02, -2.3310e-02, -4.7902e-01,\n",
            "         -6.1041e-01, -7.4806e-02, -1.8028e-01,  7.3308e-04, -2.4974e-01,\n",
            "         -2.2859e-02,  3.3835e-01, -8.3685e-02,  2.3772e-01, -4.9523e-02]],\n",
            "       grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gXqvS1QYNZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# adding few more different layers\n",
        "class Module2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(50*50,100)\n",
        "        self.l2 = nn.ReLU()\n",
        "        self.l3 = nn.Linear(100,100)\n",
        "        self.l4 = nn.Tanh()        \n",
        "        self.l5 = nn.Linear(100,10) \n",
        "        self.l6 = nn.LogSoftmax(dim=0)\n",
        "        \n",
        "    # we don't return a thing (read:None), and don't take a thing\n",
        "    def forward(self,x): \n",
        "        print(\"Module2:forward\")\n",
        "        x = self.l1(x)\n",
        "        x = self.l2(x)\n",
        "        x = self.l3(x)\n",
        "        x = self.l4(x)\n",
        "        x = self.l5(x)        \n",
        "        out = self.l6(x)\n",
        "        return out\n",
        "        \n",
        "        \n",
        "# we create a module instance m2\n",
        "m2 = Module2()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MXCzv178THE",
        "colab_type": "code",
        "outputId": "132bbcc5-bf47-4296-809a-b49c32f5d719",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# the total number of parameters grow up to 261k\n",
        "# we must be carefull with params, as they take out all memory\n",
        "t = [p.numel() for p in m2.parameters() ]\n",
        "t, sum(t)\n",
        "\n",
        "#([250000, 100, 10000, 100, 1000, 10], 261210)\n",
        "# total number of parameter is ~ 261K"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([250000, 100, 10000, 100, 1000, 10], 261210)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foIa9Wpgu-tG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "fb26f6ce-1c5e-4dad-dcdd-9334972a97ef"
      },
      "source": [
        "# like before we create the input tensor\n",
        "inp = torch.empty(10, 50*50).uniform_(-1, 1)\n",
        "print(inp)\n",
        "out = m2(inp)\n",
        "print(out.shape)\n",
        "print(out)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.4002,  0.0428, -0.3155,  ..., -0.4366,  0.5195, -0.6968],\n",
            "        [-0.0099,  0.3947, -0.9717,  ...,  0.6382, -0.5189,  0.9321],\n",
            "        [-0.9478, -0.6170, -0.0410,  ..., -0.1291, -0.8287,  0.4782],\n",
            "        ...,\n",
            "        [-0.8164,  0.3744, -0.1388,  ...,  0.7256, -0.8553, -0.8022],\n",
            "        [ 0.4623,  0.0334,  0.0570,  ...,  0.2202,  0.0360, -0.4752],\n",
            "        [ 0.9311, -0.7473, -0.1324,  ...,  0.8351,  0.3134,  0.4498]])\n",
            "Module2:forward\n",
            "torch.Size([10, 10])\n",
            "tensor([[-2.2888, -2.2168, -2.3781, -2.2525, -2.3557, -2.3032, -2.3818, -2.2191,\n",
            "         -2.2785, -2.3744],\n",
            "        [-2.2999, -2.3793, -2.1945, -2.2368, -2.3011, -2.2784, -2.2420, -2.1849,\n",
            "         -2.2610, -2.3314],\n",
            "        [-2.3537, -2.2859, -2.3454, -2.2686, -2.2640, -2.3539, -2.3098, -2.3816,\n",
            "         -2.2885, -2.2711],\n",
            "        [-2.3157, -2.3711, -2.2845, -2.3315, -2.3559, -2.2588, -2.3438, -2.2919,\n",
            "         -2.2535, -2.3107],\n",
            "        [-2.3277, -2.2396, -2.3399, -2.3055, -2.4148, -2.2994, -2.3074, -2.3485,\n",
            "         -2.3305, -2.2808],\n",
            "        [-2.2746, -2.3285, -2.1844, -2.3369, -2.4990, -2.2096, -2.3222, -2.3736,\n",
            "         -2.2757, -2.3748],\n",
            "        [-2.2756, -2.4225, -2.2833, -2.3056, -2.3040, -2.2565, -2.2744, -2.2589,\n",
            "         -2.2540, -2.2583],\n",
            "        [-2.3935, -2.2880, -2.2573, -2.4172, -2.2238, -2.3632, -2.2620, -2.3373,\n",
            "         -2.2623, -2.2454],\n",
            "        [-2.2196, -2.2288, -2.4817, -2.3029, -2.0948, -2.3221, -2.2463, -2.3315,\n",
            "         -2.4500, -2.2949],\n",
            "        [-2.2869, -2.2866, -2.3110, -2.2799, -2.2674, -2.3949, -2.3459, -2.3180,\n",
            "         -2.3911, -2.2930]], grad_fn=<LogSoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}